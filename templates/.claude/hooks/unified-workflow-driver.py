#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Unified Workflow Driver - ç»Ÿä¸€å·¥ä½œæµé©±åŠ¨å™¨ (v20.2.8)

è§¦å‘æ—¶æœº: PostToolUse (Read/Write/Edit/Bash)
èŒè´£:
1. å¿«é€Ÿæ£€æŸ¥ .task-active.json,æ— æ´»è·ƒä»»åŠ¡åˆ™è·³è¿‡
2. æ ¹æ®å·¥å…·ç±»å‹åˆ†å‘å¤„ç†
3. æ›´æ–°ä»»åŠ¡çŠ¶æ€æœº
4. æ£€æŸ¥æ­¥éª¤å®Œæˆæ¡ä»¶
5. æ³¨å…¥ä¸‹ä¸€æ­¥æŒ‡ä»¤(é˜²é‡å¤æ³¨å…¥)
6. (v20.2.7) ä¸‰æ–‡ä»¶çŠ¶æ€åŒæ­¥ï¼štask-meta.json <-> workflow-state.json <-> task-active.json
7. (v20.2.8) å¼‚å¸¸éš”ç¦»æœºåˆ¶ï¼šé˜²æ­¢å•ç‚¹æ•…éšœå¯¼è‡´Hookå®Œå…¨å¤±æ•ˆ

å˜æ›´æ—¥å¿—:
- v20.2.8 (2025-11-14):
  * ğŸ”§ P0ä¿®å¤: åˆ é™¤line 878çš„é‡å¤datetimeå¯¼å…¥,ä¿®å¤UnboundLocalError
  * ğŸ›¡ï¸ P1ä¿®å¤: æ·»åŠ å¼‚å¸¸éš”ç¦»æœºåˆ¶,å°†å…³é”®ä¸šåŠ¡é€»è¾‘åŒ…è£…åœ¨ç‹¬ç«‹try-catchå—
  * âœ… ä¿®å¤åå„é˜¶æ®µç‹¬ç«‹å¤±è´¥ä¸ä¼šå½±å“æ•´ä½“Hookæ‰§è¡Œ
  * ğŸ“ è¯¦è§: BUGä¿®å¤å·¥ä½œæµæ‰§è¡Œé—®é¢˜åˆ†ææŠ¥å‘Š-v20.2.7.md

é€€å‡ºç :
- 0: æˆåŠŸ
"""

import sys
import json
import os
from datetime import datetime
import io

# Windowsç¼–ç ä¿®å¤
if sys.platform == 'win32':
    sys.stdin = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8')
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Import logger
try:
    from hook_logger import HookLogger
except ImportError:
    class HookLogger:
        def __init__(self, name): self.name = name
        def start(self): pass
        def finish(self, success=True, message=""): pass
        def info(self, msg, data=None): pass
        def error(self, msg, err=None): pass
        def decision(self, t, r, d=None): pass

# Import notification module (v20.1)
try:
    from vscode_notify import notify_info
except ImportError:
    def notify_info(msg, detail=""): pass

def load_json(file_path):
    """åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except:
        return None

def save_json(file_path, data):
    """ä¿å­˜JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        return True
    except:
        return False

def update_docs_read(meta, file_path):
    """æ›´æ–°æ–‡æ¡£é˜…è¯»è®°å½•"""
    if not file_path.endswith('.md'):
        return False

    # æ’é™¤ä¸è®¡å…¥çš„æ–‡æ¡£
    excluded = ['README.md', 'CHANGELOG.md', u'ç´¢å¼•.md', u'é¡¹ç›®çŠ¶æ€.md', u'æ–‡æ¡£å¾…è¡¥å……æ¸…å•.md']
    if any(pattern in file_path for pattern in excluded):
        return False

    docs_read = meta.get("metrics", {}).get("docs_read", [])
    if file_path not in docs_read:
        docs_read.append(file_path)
        meta["metrics"]["docs_read"] = docs_read
        meta["metrics"]["docs_read_count"] = len(docs_read)

        # åŒæ­¥åˆ°step2_docs
        if "step2_docs" in meta["workflow_state"]["steps"]:
            meta["workflow_state"]["steps"]["step2_docs"]["docs_read"] = docs_read

        return True
    return False

def update_code_changes(meta, tool_data, cwd):
    """è®°å½•ä»£ç ä¿®æ”¹å¹¶æ›´æ–°åŒæ–‡ä»¶ç¼–è¾‘è®¡æ•° (v20.2 fix)"""
    file_path = tool_data.get("tool_input", {}).get("file_path", "")
    if not file_path:
        return False

    change_record = {
        "file": file_path,
        "timestamp": datetime.now().isoformat(),
        "operation": tool_data.get("tool_name", "Unknown"),
        "status": "success"  # v20.3: æ ‡è®°æˆåŠŸ
    }

    if "code_changes" not in meta["metrics"]:
        meta["metrics"]["code_changes"] = []

    meta["metrics"]["code_changes"].append(change_record)
    meta["metrics"]["code_changes_count"] = len(meta["metrics"]["code_changes"])

    # v20.2: ç»Ÿè®¡åŒæ–‡ä»¶ç¼–è¾‘æ¬¡æ•°ï¼ˆåŒ…å«æˆåŠŸå’Œå¤±è´¥ï¼‰
    same_file_edits = sum(1 for change in meta["metrics"]["code_changes"]
                          if change["file"] == file_path)

    # åŒæ­¥åˆ°workflow-state.jsonçš„bug_fix_tracking
    workflow_state_path = os.path.join(cwd, '.claude', 'workflow-state.json')
    workflow_state = load_json(workflow_state_path)

    if workflow_state and "bug_fix_tracking" in workflow_state:
        workflow_state["bug_fix_tracking"]["loop_indicators"]["same_file_edit_count"] = same_file_edits
        save_json(workflow_state_path, workflow_state)

        # åŒæ­¥å›meta
        meta["workflow_state"] = workflow_state

    return True


def update_failed_operations(meta, tool_data, cwd):
    """è®°å½•å¤±è´¥çš„å·¥å…·æ“ä½œ (v20.3 æ–°å¢)

    ç­–ç•¥:
    1. å¤±è´¥æ“ä½œä¹Ÿè®¡å…¥code_changesæ•°ç»„ï¼ˆæ ‡è®°status=failedï¼‰
    2. ç»Ÿè®¡same_file_edit_countï¼ˆå¤±è´¥ä¹Ÿç®—ä¸€æ¬¡å°è¯•ï¼‰
    3. æ›´æ–°consecutive_failuresï¼ˆè¿ç»­å¤±è´¥è®¡æ•°å™¨ï¼‰
    """
    file_path = tool_data.get("tool_input", {}).get("file_path", "")
    if not file_path:
        return False

    # æå–é”™è¯¯ä¿¡æ¯
    result = tool_data.get("result", {})
    error_msg = ""
    if isinstance(result, dict):
        error_msg = result.get("error", str(result))
    else:
        error_msg = str(result)[:200]

    failure_record = {
        "file": file_path,
        "timestamp": datetime.now().isoformat(),
        "operation": tool_data.get("tool_name", "Unknown"),
        "status": "failed",  # v20.3: æ ‡è®°å¤±è´¥
        "error": error_msg[:200]  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
    }

    if "code_changes" not in meta["metrics"]:
        meta["metrics"]["code_changes"] = []

    meta["metrics"]["code_changes"].append(failure_record)
    meta["metrics"]["code_changes_count"] = len(meta["metrics"]["code_changes"])

    # v20.3: ç»Ÿè®¡åŒæ–‡ä»¶ç¼–è¾‘æ¬¡æ•°ï¼ˆåŒ…å«å¤±è´¥ï¼‰
    same_file_edits = sum(1 for change in meta["metrics"]["code_changes"]
                          if change["file"] == file_path)

    # v20.3: ç»Ÿè®¡è¿ç»­å¤±è´¥æ¬¡æ•°
    recent_operations = meta["metrics"]["code_changes"][-5:]  # æœ€è¿‘5æ¬¡æ“ä½œ
    consecutive_failures = 0
    for op in reversed(recent_operations):
        if op.get("status") == "failed" and op.get("file") == file_path:
            consecutive_failures += 1
        else:
            break

    if "consecutive_failures" not in meta["metrics"]:
        meta["metrics"]["consecutive_failures"] = 0
    meta["metrics"]["consecutive_failures"] = consecutive_failures

    # åŒæ­¥åˆ°workflow-state.json
    workflow_state_path = os.path.join(cwd, '.claude', 'workflow-state.json')
    workflow_state = load_json(workflow_state_path)

    if workflow_state and "bug_fix_tracking" in workflow_state:
        workflow_state["bug_fix_tracking"]["loop_indicators"]["same_file_edit_count"] = same_file_edits
        workflow_state["bug_fix_tracking"]["loop_indicators"]["consecutive_failures"] = consecutive_failures
        save_json(workflow_state_path, workflow_state)

        # åŒæ­¥å›meta
        meta["workflow_state"] = workflow_state

    return True

def check_test_failure(result_str):
    """æ£€æµ‹æµ‹è¯•å¤±è´¥"""
    failure_indicators = [
        'error', 'Error', 'ERROR',
        'failed', 'Failed', 'FAILED',
        'traceback', 'Traceback',
        'exception', 'Exception'
    ]
    return any(indicator in result_str for indicator in failure_indicators)

# ==================== v20.2 æ–°å¢ï¼šå¾ªç¯æ£€æµ‹ä¸ä¸“å®¶è§¦å‘ ====================

def check_expert_trigger(meta, cwd):
    """
    å¾ªç¯æ£€æµ‹å™¨ - åˆ¤æ–­æ˜¯å¦è§¦å‘ä¸“å®¶å®¡æŸ¥

    è¿”å›:
    {
        "should_trigger": bool,
        "loop_type": "bug_fix_loop" | "requirement_mismatch" | None,
        "confidence": float,
        "evidence": dict
    }
    """

    # è¯»å–workflow-stateä»¥è·å–tracking_state
    workflow_state_path = os.path.join(cwd, '.claude', 'workflow-state.json')
    workflow_state = load_json(workflow_state_path)

    if not workflow_state:
        return {"should_trigger": False, "loop_type": None}

    # === Bugä¿®å¤å¾ªç¯æ£€æµ‹ ===
    if workflow_state.get("bug_fix_tracking", {}).get("enabled"):
        tracking = workflow_state["bug_fix_tracking"]
        indicators = tracking.get("loop_indicators", {})
        iterations_count = len(tracking.get("iterations", []))

        # è§¦å‘æ¡ä»¶:
        # 1. è‡³å°‘2æ¬¡è¿­ä»£
        # 2. è‡³å°‘2æ¬¡è´Ÿé¢åé¦ˆ
        # 3. è‡³å°‘2æ¬¡åŒæ–‡ä»¶ä¿®æ”¹
        negative_count = indicators.get("negative_feedback_count", 0)
        same_file_count = indicators.get("same_file_edit_count", 0)

        if (iterations_count >= 2 and
            negative_count >= 2 and
            same_file_count >= 2):

            return {
                "should_trigger": True,
                "loop_type": "bug_fix_loop",
                "confidence": 0.9,
                "evidence": {
                    "iterations": iterations_count,
                    "negative_feedback": negative_count,
                    "same_file_edits": same_file_count,
                    "pattern": u"è¡¨è±¡ä¿®å¤å¾ªç¯ - åå¤ä¿®æ”¹åŒä¸€ä½ç½®ä½†æœªè§£å†³æ ¹æœ¬é—®é¢˜"
                }
            }

    # === éœ€æ±‚å®ç°å¾ªç¯æ£€æµ‹ ===
    if workflow_state.get("feature_tracking", {}).get("enabled"):
        tracking = workflow_state["feature_tracking"]
        iterations_count = len(tracking.get("iterations", []))
        requirement_changes = len(tracking.get("requirement_changes", []))

        # è§¦å‘æ¡ä»¶:
        # 1. è‡³å°‘2æ¬¡è¿­ä»£
        # 2. è‡³å°‘2æ¬¡ä¸æ»¡æ„åé¦ˆ
        dissatisfied_count = sum(
            1 for iter in tracking.get("iterations", [])
            if iter.get("user_satisfaction") == "dissatisfied"
        )

        if (iterations_count >= 2 and dissatisfied_count >= 2):

            return {
                "should_trigger": True,
                "loop_type": "requirement_mismatch",
                "confidence": 0.85,
                "evidence": {
                    "iterations": iterations_count,
                    "dissatisfied_count": dissatisfied_count,
                    "requirement_changes": requirement_changes,
                    "pattern": u"éœ€æ±‚ç†è§£åå·® - å®ç°æ–¹å‘ä¸ç”¨æˆ·æœŸæœ›ä¸ä¸€è‡´"
                }
            }

    return {
        "should_trigger": False,
        "loop_type": None,
        "confidence": 0.0,
        "evidence": {}
    }


def launch_meta_expert(expert_check, meta, cwd, logger):
    """
    å¯åŠ¨Meta-Expertä¸“å®¶åˆ†æ

    æµç¨‹:
    1. è¯»å–å®Œæ•´çš„è¿­ä»£å†å²
    2. æ„å»ºä¸“å®¶åˆ†æprompt
    3. æ³¨å…¥åˆ°å¯¹è¯ä¸Šä¸‹æ–‡
    4. ç­‰å¾…AIç”Ÿæˆåˆ†ææŠ¥å‘Š
    """

    loop_type = expert_check["loop_type"]
    evidence = expert_check["evidence"]

    # è¯»å–workflow-stateè·å–å®Œæ•´å†å²
    workflow_state_path = os.path.join(cwd, '.claude', 'workflow-state.json')
    workflow_state = load_json(workflow_state_path)

    if not workflow_state:
        logger.error(u"æ— æ³•è¯»å–workflow-state")
        return None

    # æ„å»ºå†å²æ‘˜è¦
    history_summary = ""

    if loop_type == "bug_fix_loop":
        tracking = workflow_state.get("bug_fix_tracking", {})
        history_summary = u"## è¿­ä»£å†å²\n\n"

        for iter in tracking.get("iterations", []):
            history_summary += u"### è¿­ä»£ {}\n".format(iter["iteration_id"])
            history_summary += u"- æ—¶é—´: {}\n".format(iter["timestamp"])
            history_summary += u"- ç”¨æˆ·åé¦ˆ: {}\n".format(iter.get("user_feedback", "æ— "))
            history_summary += u"- æƒ…æ„Ÿ: {}\n".format(iter.get("feedback_sentiment", "neutral"))
            history_summary += u"- ä¿®æ”¹æ–‡ä»¶:\n"
            for change in iter.get("changes_made", []):
                history_summary += u"  - {}: {}\n".format(change["file"], change["change_summary"])
            history_summary += u"\n"

    elif loop_type == "requirement_mismatch":
        tracking = workflow_state.get("feature_tracking", {})
        history_summary = u"## éœ€æ±‚è°ƒæ•´å†å²\n\n"

        for iter in tracking.get("iterations", []):
            history_summary += u"### è¿­ä»£ {}\n".format(iter["iteration_id"])
            history_summary += u"- æ—¶é—´: {}\n".format(iter["timestamp"])
            history_summary += u"- ç”¨æˆ·æ»¡æ„åº¦: {}\n".format(iter.get("user_satisfaction", "neutral"))
            history_summary += u"- è°ƒæ•´è¯·æ±‚: {}\n\n".format(iter.get("adjustment_request", "æ— "))

    # æ„å»ºä¸“å®¶åˆ†æprompt
    expert_prompt = u"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ ä¸“å®¶å®¡æŸ¥ç³»ç»Ÿå·²è§¦å‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## æ£€æµ‹åˆ°çš„é—®é¢˜æ¨¡å¼

**å¾ªç¯ç±»å‹**: {}
**ç½®ä¿¡åº¦**: {:.0%}
**è¯æ®**:
{}

{}

## ä½ çš„ä»»åŠ¡

ä½ ç°åœ¨éœ€è¦ä»**æˆ˜ç•¥é«˜åº¦**åˆ†æé—®é¢˜ï¼Œè€Œéç»§ç»­å°è¯•ä¿®å¤ã€‚

### åœºæ™¯A: Bugä¿®å¤å¾ªç¯
å¦‚æœæ˜¯Bugä¿®å¤å¾ªç¯ï¼Œè¯·å›ç­”ï¼š

1. **æ ¹å› åˆ†æ**: ä¸ºä»€ä¹ˆåå¤ä¿®æ”¹ä»å¤±è´¥?
   - æ˜¯å¦é™·å…¥è¡¨è±¡ä¿®å¤?
   - æ˜¯å¦å­˜åœ¨æ¶æ„å±‚é¢çš„ç¼ºé™·?
   - æ˜¯å¦å¯¹é—®é¢˜çš„ç†è§£æœ‰è¯¯?

2. **å¤±è´¥æ¨¡å¼**: å†å²ä¿®æ”¹ä¸­æœ‰å“ªäº›å…±åŒçš„é”™è¯¯å‡è®¾?

3. **å¤‡é€‰è·¯å¾„**: é™¤äº†å½“å‰æ–¹å‘ï¼Œè¿˜æœ‰å“ª3-5ç§å¯èƒ½çš„è§£å†³æ€è·¯?
   - è·¯å¾„A: [åç§°] - [ä¼˜ç‚¹] - [ç¼ºç‚¹] - [é€‚ç”¨åœºæ™¯]
   - è·¯å¾„B: ...

4. **æ¨èç­–ç•¥**: æ¨èå“ªç§è·¯å¾„ï¼Œä»¥åŠå¦‚ä½•éªŒè¯?

### åœºæ™¯B: éœ€æ±‚ç†è§£åå·®
å¦‚æœæ˜¯éœ€æ±‚è°ƒæ•´å¾ªç¯ï¼Œè¯·å›ç­”ï¼š

1. **éœ€æ±‚åˆ†æ**: ç”¨æˆ·çœŸæ­£æƒ³è¦ä»€ä¹ˆ?(å¯èƒ½ä¸è¡¨é¢éœ€æ±‚ä¸åŒ)

2. **å·®è·è¯Šæ–­**: å½“å‰å®ç°ä¸ç”¨æˆ·æœŸæœ›çš„æœ¬è´¨å·®è·åœ¨å“ª?

3. **æ–¹æ¡ˆå¯¹æ¯”**: æä¾›3-5ç§å®ç°ç­–ç•¥ï¼Œæ ‡æ³¨ä¼˜ç¼ºç‚¹
   - æ–¹æ¡ˆA: [åç§°] - [ä¼˜ç‚¹] - [ç¼ºç‚¹] - [é¢„è®¡å·¥ä½œé‡]
   - æ–¹æ¡ˆB: ...

4. **æ¾„æ¸…é—®é¢˜**: åˆ—å‡ºéœ€è¦å‘ç”¨æˆ·ç¡®è®¤çš„5ä¸ªå…³é”®é—®é¢˜

## è¾“å‡ºæ ¼å¼

ä½¿ç”¨ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºï¼š

# ğŸ¯ ä¸“å®¶è¯Šæ–­æŠ¥å‘Š

## 1. é—®é¢˜æ ¹å› 

[æ·±åº¦åˆ†æ...]

## 2. å¤‡é€‰æ–¹æ¡ˆ

### æ–¹æ¡ˆA: [åç§°]
- **ä¼˜ç‚¹**: ...
- **ç¼ºç‚¹**: ...
- **é€‚ç”¨åœºæ™¯**: ...
- **é¢„è®¡å·¥ä½œé‡**: ...

### æ–¹æ¡ˆB: [åç§°]
...

## 3. æ¨èç­–ç•¥

[å…·ä½“å»ºè®®ï¼ŒåŒ…æ‹¬å®æ–½æ­¥éª¤å’ŒéªŒè¯æ–¹æ³•]

## 4. éœ€è¦å‘ç”¨æˆ·æ¾„æ¸…çš„é—®é¢˜

1. [é—®é¢˜1]
2. [é—®é¢˜2]
...

## é‡è¦æé†’

- âŒ ä¸è¦ç›´æ¥ä¿®æ”¹ä»£ç 
- âŒ ä¸è¦å‡è®¾é—®é¢˜å·²è¢«å‡†ç¡®å®šä½
- âœ… ä»é«˜ç»´åº¦åˆ†æé—®é¢˜æœ¬è´¨
- âœ… æä¾›å¤šç§å¤‡é€‰æ–¹æ¡ˆ
- âœ… æ˜ç¡®æŒ‡å‡ºéœ€è¦æ¾„æ¸…çš„ç‚¹

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¯·ç«‹å³å¼€å§‹åˆ†æã€‚
""".format(
        loop_type,
        expert_check["confidence"],
        u"\n".join([u"- {}: {}".format(k, v) for k, v in evidence.items()]),
        history_summary
    )

    # æ ‡è®°ä¸“å®¶å·²è§¦å‘
    meta["metrics"]["expert_review_triggered"] = True
    meta["metrics"]["expert_triggered_at"] = datetime.now().isoformat()

    # ä¿å­˜åˆ°.task-meta.json
    task_dir = meta.get("task_id")
    if task_dir:
        meta_path = os.path.join(cwd, 'tasks', task_dir, '.task-meta.json')
        save_json(meta_path, meta)

    logger.info(u"ä¸“å®¶å®¡æŸ¥å·²è§¦å‘", {
        "loop_type": loop_type,
        "confidence": expert_check["confidence"]
    })

    return expert_prompt

# ==================== v20.2 ç»“æŸ ====================

def check_step_completed(step_name, meta):
    """Check if workflow step is completed (v20.1: removed step2)"""
    steps = meta["workflow_state"]["steps"]

    if step_name == "step0_context":
        docs_read = meta.get("metrics", {}).get("docs_read", [])
        return any("CLAUDE.md" in doc.upper() for doc in docs_read)

    elif step_name == "step1_understand":
        return meta.get("metrics", {}).get("docs_read_count", 0) > 0

    elif step_name == "step3_execute":
        return steps["step3_execute"].get("user_confirmed", False)

    elif step_name == "step4_cleanup":
        return steps["step4_cleanup"]["status"] == "completed"

    return False

def get_next_step(current_step):
    """Get next step in workflow (v20.1: removed step2)"""
    step_order = ["step0_context", "step1_understand", "step3_execute", "step4_cleanup"]
    try:
        current_idx = step_order.index(current_step)
        if current_idx < len(step_order) - 1:
            return step_order[current_idx + 1]
    except ValueError:
        pass
    return None

def trigger_doc_update_agent(meta, cwd):
    """è§¦å‘æ–‡æ¡£æ›´æ–°å­ä»£ç† (v20.0.4)"""
    import subprocess

    logger = HookLogger("doc-update-agent-trigger")
    logger.start()

    try:
        # æ„å»ºå­ä»£ç†ä»»åŠ¡æè¿°
        task_desc = meta.get("task_description", "Unknown Task")
        docs_read = meta.get("metrics", {}).get("docs_read", [])
        code_changes = meta.get("metrics", {}).get("code_changes", [])

        agent_prompt = u"""
ğŸ¤– å­ä»£ç†ä»»åŠ¡ï¼šè‡ªåŠ¨æ–‡æ¡£æ›´æ–°ä¸æ”¶å°¾å·¥ä½œ

**ä»»åŠ¡ä¸Šä¸‹æ–‡**:
- ä¸»ä»»åŠ¡: {}
- å·²é˜…è¯»æ–‡æ¡£: {}ä¸ª
- ä»£ç ä¿®æ”¹: {}å¤„

**ä½ çš„èŒè´£**:
1. æœç´¢ markdown/ ç›®å½•ä¸‹æ‰€æœ‰åŒ…å«"å¾…è¡¥å……"æˆ–"TODO"æ ‡è®°çš„æ–‡æ¡£
2. åˆ†ææ ‡è®°çš„ä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­æ˜¯å¦ä¸æœ¬æ¬¡ä»»åŠ¡ç›¸å…³
3. å¦‚æœç›¸å…³æ–‡æ¡£â‰¤2ä¸ªï¼Œä½¿ç”¨Editå·¥å…·æ›´æ–°è¿™äº›æ–‡æ¡£çš„å†…å®¹
4. å¦‚æœç›¸å…³æ–‡æ¡£>2ä¸ªï¼Œå°†åˆ—è¡¨è¿½åŠ åˆ° markdown/æ–‡æ¡£å¾…è¡¥å……æ¸…å•.md
5. æœç´¢ä»£ç ä¸­çš„DEBUG/ä¸´æ—¶è°ƒè¯•è¯­å¥ï¼Œå»ºè®®æ¸…ç†
6. è¾“å‡ºæ”¶å°¾æŠ¥å‘Š

**æ‰§è¡Œæ­¥éª¤**:

Step 1: æœç´¢å¾…è¡¥å……æ ‡è®°
```bash
# ä½¿ç”¨Grepæœç´¢markdownç›®å½•
Grep("å¾…è¡¥å……|TODO", path="markdown/", output_mode="files_with_matches", -i=True)
```

Step 2: åˆ†æç›¸å…³æ€§
- é˜…è¯»æœç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼ˆä½¿ç”¨Grep -C 3è·å–ä¸Šä¸‹æ–‡ï¼‰
- åˆ¤æ–­æ˜¯å¦ä¸ä¸»ä»»åŠ¡"{}"ç›¸å…³

Step 3: æ‰§è¡Œæ›´æ–°
- å¦‚æœâ‰¤2ä¸ªç›¸å…³æ–‡æ¡£ï¼š
  - ä½¿ç”¨Readè¯»å–å®Œæ•´æ–‡æ¡£
  - æ ¹æ®ä»»åŠ¡å†…å®¹è¡¥å……"å¾…è¡¥å……"éƒ¨åˆ†
  - ä½¿ç”¨Editæ›´æ–°æ–‡æ¡£
- å¦‚æœ>2ä¸ªç›¸å…³æ–‡æ¡£ï¼š
  - ç”Ÿæˆå¾…è¡¥å……æ¸…å•
  - è¿½åŠ åˆ° markdown/æ–‡æ¡£å¾…è¡¥å……æ¸…å•.md

Step 4: DEBUG clean check
```bash
Grep("DEBUG|print.*debug|console.log.*test", path=".", glob="*.py", output_mode="content", -n=True)
```

Step 5: Mark step4 completed (CRITICAL - triggers archive)
Use Write tool to update task metadata:
1. Read latest task's .task-meta.json
2. Set workflow_state.steps.step4_cleanup.status = "completed"
3. Write back the updated JSON

Step 6: Output completion report
Format:
```
==============================================
Cleanup Work Completion Report
==============================================

Docs updated: [file list]
DEBUG cleanup: [count] found, [locations]
Task archive: tasks/task-XXX/
  - context.md: OK
  - solution.md: OK
Step4 marked: COMPLETED (archive hook will trigger)
==============================================
```

**IMPORTANT**:
- You are an independent subagent with full tool access
- Do NOT ask user, complete all steps autonomously
- MUST execute Step 5 to mark step4_cleanup completed
- After marking, post-archive-hook will auto-move task directory
- Keep output concise, focus on execution
""".format(
            task_desc,
            len(docs_read),
            len(code_changes),
            task_desc
        )

        # å†™å…¥ä¸´æ—¶ä»»åŠ¡æ–‡ä»¶
        agent_task_file = os.path.join(cwd, '.claude', '.agent-doc-update.txt')
        with open(agent_task_file, 'w', encoding='utf-8') as f:
            f.write(agent_prompt)

        logger.info(u"å­ä»£ç†ä»»åŠ¡æ–‡ä»¶å·²åˆ›å»º", {"file": agent_task_file})

        # æ³¨å…¥å­ä»£ç†è°ƒç”¨æŒ‡ä»¤
        message = u"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¤– å¯åŠ¨æ–‡æ¡£æ›´æ–°å­ä»£ç†
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ£€æµ‹åˆ°æ­¥éª¤4ï¼ˆæ”¶å°¾å½’æ¡£ï¼‰å¼€å§‹ï¼Œç³»ç»Ÿå°†å¯åŠ¨ä¸“é—¨çš„å­ä»£ç†å®Œæˆä»¥ä¸‹å·¥ä½œï¼š
- ğŸ“ è‡ªåŠ¨æ›´æ–°markdownæ–‡æ¡£ä¸­çš„"å¾…è¡¥å……"å†…å®¹
- ğŸ§¹ æ£€æŸ¥å¹¶æ¸…ç†DEBUGä»£ç 
- ğŸ“¦ æ•´ç†ä»»åŠ¡å½’æ¡£

**è¯·ä½¿ç”¨Taskå·¥å…·å¯åŠ¨å­ä»£ç†**ï¼š

```
Task(
    subagent_type="general-purpose",
    description="æ–‡æ¡£æ›´æ–°ä¸æ”¶å°¾å·¥ä½œ",
    prompt=Read("{}").content
)
```

å­ä»£ç†å°†ç‹¬ç«‹å®Œæˆæ‰€æœ‰æ”¶å°¾å·¥ä½œï¼Œä¸æ¶ˆè€—ä¸»ä¼šè¯ä¸Šä¸‹æ–‡ã€‚
å®Œæˆåä¼šè¾“å‡ºè¯¦ç»†æŠ¥å‘Šã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""".format(agent_task_file)

        logger.finish(success=True, message=u"å­ä»£ç†è§¦å‘æŒ‡ä»¤å·²æ³¨å…¥")
        return message

    except Exception as e:
        logger.error(u"å­ä»£ç†è§¦å‘å¤±è´¥", e)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return None

def inject_next_step_prompt(next_step, meta, cwd=None):
    """æ³¨å…¥ä¸‹ä¸€æ­¥æŒ‡ä»¤"""
    step_config = meta["workflow_state"]["steps"][next_step]

    # ç‰¹æ®Šå¤„ç†ï¼šæ­¥éª¤4å¯åŠ¨å­ä»£ç†
    if next_step == "step4_cleanup" and cwd:
        # v20.2.7: å…ˆä»ä¼šè¯å†å²ç”Ÿæˆcontext.mdå’Œsolution.md
        task_id = meta.get("task_id")
        if task_id:
            task_dir = os.path.join(cwd, 'tasks', task_id)
            conversation_file = os.path.join(task_dir, '.conversation.jsonl')

            # æ£€æŸ¥ä¼šè¯å†å²æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(conversation_file):
                try:
                    import subprocess
                    # è°ƒç”¨ç”Ÿæˆè„šæœ¬
                    result = subprocess.run(
                        [sys.executable, '.claude/hooks/generate-docs-from-conversation.py', task_dir],
                        cwd=cwd,
                        capture_output=True,
                        text=True,
                        timeout=10,
                        encoding='utf-8'
                    )

                    if result.returncode == 0:
                        sys.stderr.write(u"[INFO] âœ… å·²ä»ä¼šè¯å†å²ç”Ÿæˆcontext.mdå’Œsolution.md\n")
                        sys.stderr.write(result.stdout)
                    else:
                        sys.stderr.write(u"[WARN] ç”Ÿæˆæ–‡æ¡£å¤±è´¥:\n{}\n".format(result.stderr))
                except Exception as e:
                    sys.stderr.write(u"[WARN] è°ƒç”¨ç”Ÿæˆè„šæœ¬å¤±è´¥: {}\n".format(e))
            else:
                sys.stderr.write(u"[WARN] ä¼šè¯å†å²æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ–‡æ¡£ç”Ÿæˆ\n")

        # ç»§ç»­å¯åŠ¨å­ä»£ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        agent_message = trigger_doc_update_agent(meta, cwd)
        if agent_message:
            output = {
                "continue": True,
                "hookSpecificOutput": {
                    "hookEventName": "PostToolUse",
                    "additionalContext": agent_message
                }
            }
            print(json.dumps(output, ensure_ascii=False))
            return

    # å¸¸è§„æ­¥éª¤ï¼šæ³¨å…¥æç¤º
    message = u"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ å·¥ä½œæµæé†’: {}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""".format(step_config.get("description", next_step), step_config["prompt"])

    output = {
        "continue": True,
        "hookSpecificOutput": {
            "hookEventName": "PostToolUse",
            "additionalContext": message
        }
    }

    print(json.dumps(output, ensure_ascii=False))

def detect_failure_pattern(meta):
    """Detect failure patterns (v20.1)"""
    failures = meta["metrics"].get("failures", [])

    if len(failures) < 2:
        return "single_failure"

    recent_failures = failures[-3:]
    error_messages = [f.get("error", "")[:100] for f in recent_failures]

    if len(set(error_messages)) == 1:
        return "repeated_same_error"

    return "varied_errors"

def should_trigger_expert_diagnosis(meta):
    """Check if expert diagnosis should be triggered (v20.1)"""

    if meta["metrics"]["expert_review_triggered"]:
        return False, None

    task_type = meta.get("task_type", "feature")
    complexity = meta.get("task_complexity", "standard")
    failure_count = meta["metrics"]["failure_count"]
    failure_pattern = detect_failure_pattern(meta)

    if failure_pattern == "repeated_same_error" and failure_count >= 3:
        return True, "Repeated same error 3 times"

    if task_type in ["feature", "bugfix"] and failure_count >= 5:
        return True, "Task failed {} times".format(failure_count)

    if task_type == "feature":
        critical_count = meta["metrics"].get("critical_violation_count", 0)
        if critical_count >= 4:
            return True, "CRITICAL violations {} times, may misunderstand rules".format(critical_count)

    if task_type == "bugfix" and failure_count >= 3:
        return True, "Bugfix failed 3 times, may be mislocated"

    if complexity == "complex":
        time_in_step3 = meta["workflow_state"]["steps"]["step3_execute"].get("elapsed_minutes", 0)
        if time_in_step3 >= 30 and failure_count >= 2:
            return True, "Complex task taking too long with difficulties"

    return False, None

def inject_expert_review_warning(meta, trigger_reason=None):
    """Inject expert review warning (v20.1: enhanced)"""
    task_desc = meta["task_description"]
    failure_count = meta["metrics"]["failure_count"]
    last_error = meta["workflow_state"]["steps"]["step3_execute"].get("last_error", u"Unknown error")
    reason = trigger_reason if trigger_reason else "Multiple failures detected"

    sys.stderr.write(u"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸  Warning: Task encountering difficulties, expert review recommended
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Task**: {}
**Failure Count**: {} times
**Trigger Reason**: {}
**Recent Error**: {}

**Expert Review Process**:
1. System will inject detailed debugging context
2. AI will re-analyze root cause
3. Provide deeper solution

**Continue current approach?**
- Input "trigger expert review" â†’ Start deep analysis
- Input "continue trying" â†’ Keep current approach

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""".format(task_desc, failure_count, reason, last_error))

def main():
    logger = HookLogger("unified-workflow-driver")
    logger.start()

    try:
        # è¯»å–stdinè¾“å…¥
        data = json.load(sys.stdin)
        tool_name = data.get('tool_name', '')
        cwd = os.environ.get('CLAUDE_PROJECT_DIR', os.getcwd())

        # å¿«é€Ÿæ£€æŸ¥: æ˜¯å¦æœ‰æ´»è·ƒä»»åŠ¡
        active_flag_path = os.path.join(cwd, '.claude', '.task-active.json')
        if not os.path.exists(active_flag_path):
            logger.decision("skip", u"æ— æ´»è·ƒä»»åŠ¡,è·³è¿‡")
            output = {"continue": True}
            print(json.dumps(output, ensure_ascii=False))
            logger.finish(success=True, message=u"è·³è¿‡")
            sys.exit(0)

        # åŠ è½½æ´»è·ƒä»»åŠ¡
        active_flag = load_json(active_flag_path)
        if not active_flag:
            logger.error(u"æ´»è·ƒä»»åŠ¡æ ‡å¿—æŸå")
            output = {"continue": True}
            print(json.dumps(output, ensure_ascii=False))
            sys.exit(0)

        task_dir = active_flag["task_dir"]
        meta_path = os.path.join(task_dir, '.task-meta.json')
        meta = load_json(meta_path)

        if not meta:
            logger.error(u"ä»»åŠ¡å…ƒæ•°æ®æŸå")
            output = {"continue": True}
            print(json.dumps(output, ensure_ascii=False))
            sys.exit(0)

        current_step = meta["workflow_state"]["current_step"]
        last_injection = meta["workflow_state"].get("last_injection_step", None)

        logger.info(u"å¤„ç†å·¥å…·è°ƒç”¨", {
            "tool": tool_name,
            "task_id": meta["task_id"],
            "current_step": current_step
        })

        # å·¥å…·åˆ†å‘å¤„ç† (v20.2.8: æ·»åŠ å¼‚å¸¸éš”ç¦»,é˜²æ­¢å•ç‚¹æ•…éšœ)
        step_changed = False

        # === é˜¶æ®µ1: å·¥å…·ç‰¹å®šå¤„ç† (ç‹¬ç«‹å¼‚å¸¸å¤„ç†) ===
        if tool_name == "Read":
            try:
                # æ›´æ–°æ–‡æ¡£é˜…è¯»è®°å½•
                file_path = data.get('tool_input', {}).get('file_path', '')
                if update_docs_read(meta, file_path):
                    logger.info(u"æ–‡æ¡£é˜…è¯»è®°å½•å·²æ›´æ–°", {"file": file_path})
                    step_changed = check_step_completed(current_step, meta)
            except Exception as read_err:
                logger.error(u"Readå·¥å…·å¤„ç†å¤±è´¥", read_err)
                # ç»§ç»­æ‰§è¡Œå…¶ä»–é€»è¾‘,ä¸ä¸­æ–­æµç¨‹

        elif tool_name in ["Write", "Edit"]:
            try:
                # v20.3: åˆ¤æ–­å·¥å…·æ‰§è¡ŒçŠ¶æ€ï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰
                tool_result = data.get('result', {})
                is_error = False

                # åˆ¤æ–­å¤±è´¥çš„å¤šç§æƒ…å†µ
                if isinstance(tool_result, dict):
                    # æƒ…å†µ1: resultåŒ…å«errorå­—æ®µ
                    is_error = 'error' in tool_result

                # æƒ…å†µ2: resultæ˜¯å­—ç¬¦ä¸²ä¸”åŒ…å«Errorå…³é”®è¯
                result_str = str(tool_result).lower()
                if 'error' in result_str or 'failed' in result_str:
                    is_error = True

                if is_error:
                    # è®°å½•å¤±è´¥æ“ä½œ (v20.3)
                    if update_failed_operations(meta, data, cwd):
                        logger.info(u"å¤±è´¥æ“ä½œå·²è®°å½•", {
                            "file": data.get("tool_input", {}).get("file_path", ""),
                            "consecutive": meta["metrics"].get("consecutive_failures", 0)
                        })

                        # è¿ç»­å¤±è´¥â‰¥3æ¬¡ï¼Œè§¦å‘ä¸“å®¶æ£€æµ‹
                        if meta["metrics"].get("consecutive_failures", 0) >= 3:
                            expert_check = check_expert_trigger(meta, cwd)
                            if expert_check["should_trigger"]:
                                expert_prompt = launch_meta_expert(expert_check, meta, cwd, logger)
                                if expert_prompt:
                                    output = {
                                        "continue": True,
                                        "hookSpecificOutput": {
                                            "hookEventName": "PostToolUse",
                                            "additionalContext": expert_prompt
                                        }
                                    }
                                    print(json.dumps(output, ensure_ascii=False))
                                    logger.finish(success=True, message=u"è¿ç»­å¤±è´¥è§¦å‘ä¸“å®¶")
                                    sys.exit(0)
                else:
                    # è®°å½•æˆåŠŸçš„ä»£ç ä¿®æ”¹ (v20.2: åŒ…å«åŒæ–‡ä»¶ç¼–è¾‘è®¡æ•°)
                    if update_code_changes(meta, data, cwd):
                        logger.info(u"ä»£ç ä¿®æ”¹å·²è®°å½•")

                        # v20.2.7: ä¸»åŠ¨å¼•å¯¼ - ä¿®å¤å®Œæˆåæé†’AIè¯¢é—®ç”¨æˆ·æµ‹è¯•ç»“æœ
                        if current_step == "step3_execute":
                            task_type = meta.get("task_type", "general")
                            user_confirmed = meta["workflow_state"]["steps"]["step3_execute"].get("user_confirmed", False)

                            # ä»…åœ¨BUGä¿®å¤ä»»åŠ¡ + ç”¨æˆ·æœªç¡®è®¤ + æœ‰ä»£ç ä¿®æ”¹æ—¶æ³¨å…¥æé†’
                            if task_type == "bug_fix" and not user_confirmed:
                                code_changes_count = meta["metrics"].get("code_changes_count", 0)

                                # ç­–ç•¥ï¼šä»£ç ä¿®æ”¹â‰¥2æ¬¡åå¼€å§‹æé†’ï¼ˆé¿å…é¦–æ¬¡ä¿®æ”¹å°±æé†’ï¼‰
                                if code_changes_count >= 2:
                                    # æ£€æŸ¥æœ€è¿‘ä¸€æ¬¡æé†’æ—¶é—´ï¼ˆé¿å…é¢‘ç¹æé†’ï¼‰
                                    last_reminder_at = meta["workflow_state"]["steps"]["step3_execute"].get("last_test_reminder_at", None)
                                    should_remind = True

                                    if last_reminder_at:
                                        # ä½¿ç”¨å…¨å±€å¯¼å…¥çš„datetime,é¿å…å±€éƒ¨å˜é‡è¦†ç›–å¯¼è‡´UnboundLocalError
                                        last_time = datetime.fromisoformat(last_reminder_at)
                                        elapsed_minutes = (datetime.now() - last_time).total_seconds() / 60
                                        # 10åˆ†é’Ÿå†…ä¸é‡å¤æé†’
                                        if elapsed_minutes < 10:
                                            should_remind = False

                                    if should_remind:
                                        meta["workflow_state"]["steps"]["step3_execute"]["last_test_reminder_at"] = datetime.now().isoformat()
                                        save_json(meta_path, meta)

                                        reminder_message = u"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ **ä¿®å¤æé†’ï¼šè¯·å¼•å¯¼ç”¨æˆ·æµ‹è¯•éªŒè¯**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä½ å·²å®Œæˆä»£ç ä¿®æ”¹ï¼ˆå…±{}æ¬¡ï¼‰ï¼Œå»ºè®®ï¼š

1. **è¾“å‡ºä¿®å¤æ‘˜è¦** - å‘Šè¯‰ç”¨æˆ·ä½ åšäº†ä»€ä¹ˆä¿®æ”¹
2. **ä¸»åŠ¨è¯¢é—®æµ‹è¯•ç»“æœ** - "è¯·åœ¨æ¸¸æˆä¸­æµ‹è¯•éªŒè¯ï¼Œå¹¶å‘Šè¯‰æˆ‘ç»“æœ"
3. **ç­‰å¾…ç”¨æˆ·åé¦ˆ** - ä¸è¦åœ¨ç”¨æˆ·æœªç¡®è®¤å‰å°è¯•ç»“æŸä¼šè¯

**æ ‡å‡†è¯¢é—®æ¨¡æ¿**:
```
ä¿®å¤å®Œæˆï¼æˆ‘å·²ä¿®æ”¹äº† [æ–‡ä»¶å]ã€‚
è¯·åœ¨æ¸¸æˆä¸­æµ‹è¯•éªŒè¯ï¼Œå¹¶å‘Šè¯‰æˆ‘ç»“æœï¼š
- å¦‚æœé—®é¢˜å·²è§£å†³ï¼Œè¯·å›å¤"å·²ä¿®å¤"
- å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·è¯¦ç»†æè¿°é—®é¢˜ç°è±¡
```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""".format(code_changes_count)

                                        output = {
                                            "continue": True,
                                            "hookSpecificOutput": {
                                                "hookEventName": "PostToolUse",
                                                "additionalContext": reminder_message
                                            }
                                        }
                                        print(json.dumps(output, ensure_ascii=False))
                                        logger.info(u"âœ… å·²æ³¨å…¥æµ‹è¯•æé†’", {
                                            "code_changes_count": code_changes_count
                                        })
                                        sys.exit(0)
            except Exception as write_edit_err:
                logger.error(u"Write/Editå·¥å…·å¤„ç†å¤±è´¥", write_edit_err)
                # ç»§ç»­æ‰§è¡Œå…¶ä»–é€»è¾‘,ä¸ä¸­æ–­æµç¨‹

        elif tool_name == "Bash":
            try:
                # æ£€æµ‹æµ‹è¯•ç»“æœ
                result = data.get('result', {})
                result_str = json.dumps(result, ensure_ascii=False)

                if check_test_failure(result_str):
                    meta["metrics"]["failure_count"] += 1

                    # è®°å½•é”™è¯¯
                    if "failures" not in meta["metrics"]:
                        meta["metrics"]["failures"] = []

                    error_record = {
                        "timestamp": datetime.now().isoformat(),
                        "error": result_str[:500],  # é™åˆ¶é•¿åº¦
                        "command": data.get('tool_input', {}).get('command', '')
                    }
                    meta["metrics"]["failures"].append(error_record)

                    # æ›´æ–°step3çš„last_error
                    if current_step == "step3_execute":
                        meta["workflow_state"]["steps"]["step3_execute"]["last_error"] = result_str[:200]
                        meta["workflow_state"]["steps"]["step3_execute"]["last_error_time"] = datetime.now().isoformat()

                    logger.info(u"Detected test failure", {
                        "failure_count": meta["metrics"]["failure_count"]
                    })

                    # v20.1: Enhanced expert diagnosis trigger logic
                    should_trigger, trigger_reason = should_trigger_expert_diagnosis(meta)

                    if should_trigger:
                        inject_expert_review_warning(meta, trigger_reason)
                        meta["metrics"]["expert_review_triggered"] = True

                        # Desktop notification
                        try:
                            from vscode_notify import notify_warning
                            notify_warning("Expert diagnosis recommended", trigger_reason)
                        except:
                            pass

                        step_changed = True
            except Exception as bash_err:
                logger.error(u"Bashå·¥å…·å¤„ç†å¤±è´¥", bash_err)
                # ç»§ç»­æ‰§è¡Œå…¶ä»–é€»è¾‘,ä¸ä¸­æ–­æµç¨‹

        # === é˜¶æ®µ2: çŠ¶æ€æ›´æ–° (å¿…é¡»æˆåŠŸ) ===
        try:
            meta["updated_at"] = datetime.now().isoformat()
        except Exception as timestamp_err:
            logger.error(u"æ—¶é—´æˆ³æ›´æ–°å¤±è´¥", timestamp_err)
            # æ—¶é—´æˆ³æ›´æ–°å¤±è´¥ä¸è‡´å‘½,ä½¿ç”¨æ—§æ—¶é—´æˆ³ç»§ç»­

        # === é˜¶æ®µ3: æ­¥éª¤æ£€æŸ¥ä¸æ¨è¿› (ç‹¬ç«‹å¼‚å¸¸å¤„ç†) ===
        try:
            # æ£€æŸ¥æ­¥éª¤æ˜¯å¦åˆšåˆšå®Œæˆ
            if step_changed or check_step_completed(current_step, meta):
                if meta["workflow_state"]["steps"][current_step]["status"] != "completed":
                    # æ ‡è®°å½“å‰æ­¥éª¤å®Œæˆ
                    meta["workflow_state"]["steps"][current_step]["status"] = "completed"
                    meta["workflow_state"]["steps"][current_step]["completed_at"] = datetime.now().isoformat()

                    # è·å–ä¸‹ä¸€æ­¥
                    next_step = get_next_step(current_step)

                    if next_step and current_step != last_injection:
                        # æ›´æ–°çŠ¶æ€æœº
                        meta["workflow_state"]["current_step"] = next_step
                        meta["workflow_state"]["steps"][next_step]["status"] = "in_progress"
                        meta["workflow_state"]["last_injection_step"] = next_step

                        # ä¿å­˜çŠ¶æ€æœº
                        save_json(meta_path, meta)
                        save_json(active_flag_path, {
                            **active_flag,
                            "current_step": next_step,
                            "updated_at": datetime.now().isoformat()
                        })

                        # v20.2.7: åŒæ­¥åˆ° workflow-state.jsonï¼ˆP0ä¿®å¤ï¼‰
                        workflow_state_path = os.path.join(cwd, '.claude', 'workflow-state.json')
                        workflow_state = load_json(workflow_state_path)
                        if workflow_state:
                            # å®Œæ•´åŒæ­¥ steps å¯¹è±¡
                            workflow_state['current_step'] = next_step
                            workflow_state['steps'] = meta['workflow_state']['steps'].copy()
                            workflow_state['last_sync_at'] = datetime.now().isoformat()
                            if save_json(workflow_state_path, workflow_state):
                                logger.info(u"âœ… å·²åŒæ­¥åˆ°workflow-state.json", {
                                    "current_step": next_step,
                                    "steps_synced": list(workflow_state['steps'].keys())
                                })
                            else:
                                logger.error(u"âŒ workflow-state.jsonåŒæ­¥å¤±è´¥")

                        # v20.1: Desktop notification - Step completed
                        next_step_desc = meta["workflow_state"]["steps"][next_step].get("description", next_step)
                        notify_info(
                            "Step completed",
                            "Current: {} â†’ Next: {}".format(current_step, next_step_desc)
                        )

                        # Inject next step prompt
                        inject_next_step_prompt(next_step, meta, cwd)

                        logger.info(u"Step completed", {
                            "from": current_step,
                            "to": next_step
                        })
                        logger.finish(success=True, message=u"æ­¥éª¤{}å®Œæˆ,è¿›å…¥{}".format(current_step, next_step))
                        sys.exit(0)
        except Exception as step_check_err:
            logger.error(u"æ­¥éª¤æ£€æŸ¥å¤±è´¥", step_check_err)
            # æ­¥éª¤æ£€æŸ¥å¤±è´¥ä¸è‡´å‘½,ç»§ç»­æ‰§è¡Œå…¶ä»–é€»è¾‘

        # === é˜¶æ®µ4: å¾ªç¯æ£€æµ‹ä¸ä¸“å®¶è§¦å‘ (ç‹¬ç«‹å¼‚å¸¸å¤„ç†) ===
        try:
            expert_check = check_expert_trigger(meta, cwd)

            if expert_check["should_trigger"] and not meta["metrics"].get("expert_review_triggered", False):
                # è§¦å‘ä¸“å®¶å®¡æŸ¥
                expert_prompt = launch_meta_expert(expert_check, meta, cwd, logger)

                if expert_prompt:
                    # æ³¨å…¥ä¸“å®¶åˆ†ææç¤º
                    output = {
                        "continue": True,
                        "hookSpecificOutput": {
                            "hookEventName": "PostToolUse",
                            "additionalContext": expert_prompt
                        }
                    }
                    print(json.dumps(output, ensure_ascii=False))

                    # æ¡Œé¢é€šçŸ¥
                    try:
                        from vscode_notify import notify_warning
                        notify_warning(
                            u"ä¸“å®¶å®¡æŸ¥å·²è§¦å‘",
                            u"å¾ªç¯ç±»å‹: {} | ç½®ä¿¡åº¦: {:.0%}".format(
                                expert_check["loop_type"],
                                expert_check["confidence"]
                            )
                        )
                    except:
                        pass

                    logger.finish(success=True, message=u"ä¸“å®¶å®¡æŸ¥å·²è§¦å‘")
                    sys.exit(0)
        except Exception as expert_err:
            logger.error(u"ä¸“å®¶è§¦å‘æ£€æŸ¥å¤±è´¥", expert_err)
            # ä¸“å®¶è§¦å‘å¤±è´¥ä¸å½±å“ä¸»æµç¨‹

        # === é˜¶æ®µ5: çŠ¶æ€ä¿å­˜ (å¿…é¡»æˆåŠŸ,å¦åˆ™çŠ¶æ€ä¸¢å¤±) ===
        try:
            save_json(meta_path, meta)
        except Exception as save_err:
            logger.error(u"çŠ¶æ€ä¿å­˜å¤±è´¥", save_err)
            # çŠ¶æ€ä¿å­˜å¤±è´¥è§†ä¸ºä¸¥é‡é”™è¯¯,ä½†ä»ç„¶æ”¾è¡Œ
            # (é¿å…é˜»å¡ç”¨æˆ·æ“ä½œ,çŠ¶æ€å¯ä»å…¶ä»–æºæ¢å¤)

        # å½“å‰æ­¥éª¤æœªå®Œæˆ,æ”¾è¡Œ
        logger.decision("continue", u"æ­¥éª¤{}è¿›è¡Œä¸­".format(current_step))
        output = {"continue": True}
        print(json.dumps(output, ensure_ascii=False))
        logger.finish(success=True, message=u"æ”¾è¡Œ")
        sys.exit(0)

    except Exception as e:
        logger.error(u"Hookæ‰§è¡Œå¤±è´¥", e)
        import traceback
        traceback.print_exc(file=sys.stderr)
        output = {"continue": True}
        print(json.dumps(output, ensure_ascii=False))
        logger.finish(success=False, message=u"æ‰§è¡Œå¼‚å¸¸")
        sys.exit(0)

if __name__ == "__main__":
    main()
