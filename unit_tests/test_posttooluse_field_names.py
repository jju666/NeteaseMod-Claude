#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PostToolUse Hook å­—æ®µåä¿®å¤éªŒè¯æµ‹è¯•
ç”¨äºéªŒè¯ BUG ä¿®å¤ï¼šposttooluse_updater.py åº”ä½¿ç”¨ snake_case å­—æ®µå

æµ‹è¯•ç›®æ ‡:
1. éªŒè¯ PostToolUse Hook èƒ½æ­£ç¡®è¯»å– snake_case å­—æ®µåï¼ˆtool_name, tool_inputç­‰ï¼‰
2. éªŒè¯ Read å·¥å…·è°ƒç”¨åï¼Œmetrics.docs_read æ­£å¸¸æ›´æ–°
3. éªŒè¯å‘åå…¼å®¹ï¼šæ—§ç‰ˆæœ¬ camelCase å­—æ®µåä»èƒ½å·¥ä½œï¼ˆä½†ä¼šæœ‰è­¦å‘Šï¼‰
"""

import json
import os
import sys
import subprocess
import tempfile
from pathlib import Path

# Windows UTF-8 æ”¯æŒ
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# æµ‹è¯•é…ç½®
HOOK_SCRIPT = Path(__file__).parent.parent / "templates" / ".claude" / "hooks" / "orchestrator" / "posttooluse_updater.py"
TEST_TASK_DIR = Path(tempfile.gettempdir()) / "modsdk_test_task"


def setup_test_environment():
    """åˆ›å»ºæµ‹è¯•ç¯å¢ƒ"""
    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡ç›®å½•
    tasks_dir = TEST_TASK_DIR / "tasks" / "test-task-001"
    tasks_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºåˆå§‹ task-meta.json
    task_meta = {
        "task_id": "test-task-001",
        "task_type": "general",
        "current_step": "implementation",
        "architecture_version": "v3.0 Final",
        "steps": {
            "activation": {"status": "completed"},
            "planning": {
                "status": "completed",
                "required_doc_count": 3
            },
            "implementation": {"status": "in_progress"},
            "finalization": {"status": "pending"}
        },
        "metrics": {
            "tools_used": [],
            "code_changes": [],
            "docs_read": [],
            "failed_operations": []
        }
    }

    meta_path = tasks_dir / ".task-meta.json"
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(task_meta, f, ensure_ascii=False, indent=2)

    # åˆ›å»º .task-active.json
    active_path = TEST_TASK_DIR / ".claude" / ".task-active.json"
    active_path.parent.mkdir(parents=True, exist_ok=True)
    with open(active_path, 'w', encoding='utf-8') as f:
        json.dump({"task_id": "test-task-001"}, f)

    return task_meta


def cleanup_test_environment():
    """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
    import shutil
    if TEST_TASK_DIR.exists():
        shutil.rmtree(TEST_TASK_DIR)


def run_hook(hook_input_data, cwd=None):
    """è¿è¡Œ PostToolUse Hook å¹¶è¿”å›ç»“æœ"""
    if cwd is None:
        cwd = str(TEST_TASK_DIR)

    result = subprocess.run(
        ["python", str(HOOK_SCRIPT)],
        input=json.dumps(hook_input_data),
        capture_output=True,
        text=True,
        cwd=cwd,
        encoding='utf-8'
    )

    return {
        "stdout": result.stdout,
        "stderr": result.stderr,
        "returncode": result.returncode
    }


def load_task_meta():
    """è¯»å– task-meta.json"""
    meta_path = TEST_TASK_DIR / "tasks" / "test-task-001" / ".task-meta.json"
    with open(meta_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def test_snake_case_fields():
    """æµ‹è¯•1: éªŒè¯æ­£ç¡®çš„ snake_case å­—æ®µå"""
    print("\nã€æµ‹è¯•1ã€‘éªŒè¯ snake_case å­—æ®µåï¼ˆä¿®å¤åçš„æ­£ç¡®æ ¼å¼ï¼‰")
    print("=" * 60)

    # æ¨¡æ‹Ÿ PostToolUse Hook è¾“å…¥ï¼ˆä½¿ç”¨ snake_caseï¼‰
    hook_input = {
        "session_id": "test-session",
        "transcript_path": "/tmp/transcript.jsonl",
        "cwd": str(TEST_TASK_DIR),
        "permission_mode": "default",
        "hook_event_name": "PostToolUse",
        "tool_name": "Read",  # âœ… æ­£ç¡®ï¼šsnake_case
        "tool_input": {      # âœ… æ­£ç¡®ï¼šsnake_case
            "file_path": "markdown/å¼€å‘è§„èŒƒ.md"
        },
        "tool_response": {   # âœ… æ­£ç¡®ï¼štool_response
            "success": True
        },
        "is_error": False    # âœ… æ­£ç¡®ï¼šsnake_case
    }

    result = run_hook(hook_input)

    # éªŒè¯ Hook æ‰§è¡ŒæˆåŠŸ
    assert result["returncode"] == 0, f"Hookæ‰§è¡Œå¤±è´¥: {result['stderr']}"
    print("âœ… Hookæ‰§è¡ŒæˆåŠŸï¼ˆé€€å‡ºç 0ï¼‰")

    # éªŒè¯æ²¡æœ‰é”™è¯¯æ—¥å¿—
    assert "ERROR" not in result["stderr"], f"å‘ç°é”™è¯¯æ—¥å¿—: {result['stderr']}"
    print("âœ… æ— é”™è¯¯æ—¥å¿—")

    # éªŒè¯ metrics.docs_read è¢«æ›´æ–°
    task_meta = load_task_meta()
    docs_read = task_meta["metrics"]["docs_read"]

    assert len(docs_read) == 1, f"docs_read åº”è¯¥åŒ…å«1æ¡è®°å½•ï¼Œå®é™…: {len(docs_read)}"
    assert docs_read[0]["file"] == "markdown/å¼€å‘è§„èŒƒ.md", f"æ–‡ä»¶è·¯å¾„ä¸åŒ¹é…: {docs_read[0]['file']}"
    print(f"âœ… docs_read æ­£ç¡®æ›´æ–°: {docs_read[0]['file']}")

    # éªŒè¯ tools_used ä¹Ÿè¢«æ›´æ–°
    tools_used = task_meta["metrics"]["tools_used"]
    assert len(tools_used) == 1, f"tools_used åº”è¯¥åŒ…å«1æ¡è®°å½•"
    assert tools_used[0]["tool"] == "Read", f"å·¥å…·åä¸åŒ¹é…"
    print(f"âœ… tools_used æ­£ç¡®æ›´æ–°")

    print("\nã€æµ‹è¯•1é€šè¿‡ã€‘âœ… snake_case å­—æ®µåå·¥ä½œæ­£å¸¸\n")
    return True


def test_camelcase_backward_compatibility():
    """æµ‹è¯•2: éªŒè¯å‘åå…¼å®¹æ€§ï¼ˆæ—§ç‰ˆæœ¬ camelCase å­—æ®µåï¼‰"""
    print("\nã€æµ‹è¯•2ã€‘éªŒè¯å‘åå…¼å®¹æ€§ï¼ˆcamelCase å­—æ®µååº”è¯¥ä»èƒ½å·¥ä½œï¼‰")
    print("=" * 60)

    # é‡ç½®æµ‹è¯•ç¯å¢ƒ
    cleanup_test_environment()
    setup_test_environment()

    # æ¨¡æ‹Ÿæ—§ç‰ˆæœ¬ PostToolUse Hook è¾“å…¥ï¼ˆä½¿ç”¨ camelCaseï¼‰
    hook_input = {
        "session_id": "test-session",
        "transcript_path": "/tmp/transcript.jsonl",
        "cwd": str(TEST_TASK_DIR),
        "permission_mode": "default",
        "hook_event_name": "PostToolUse",
        "toolName": "Read",     # âš ï¸ æ—§ç‰ˆæœ¬ï¼šcamelCase
        "toolInput": {          # âš ï¸ æ—§ç‰ˆæœ¬ï¼šcamelCase
            "file_path": "markdown/é—®é¢˜æ’æŸ¥.md"
        },
        "toolResult": {         # âš ï¸ æ—§ç‰ˆæœ¬ï¼štoolResult
            "success": True
        },
        "isError": False        # âš ï¸ æ—§ç‰ˆæœ¬ï¼šcamelCase
    }

    result = run_hook(hook_input)

    # éªŒè¯ Hook æ‰§è¡ŒæˆåŠŸï¼ˆåº”è¯¥é€šè¿‡å®¹é”™é€»è¾‘ï¼‰
    assert result["returncode"] == 0, f"Hookæ‰§è¡Œå¤±è´¥: {result['stderr']}"
    print("âœ… Hookæ‰§è¡ŒæˆåŠŸï¼ˆå®¹é”™é€»è¾‘ç”Ÿæ•ˆï¼‰")

    # éªŒè¯æœ‰è­¦å‘Šæ—¥å¿—ï¼ˆæç¤ºä½¿ç”¨äº†æ—§ç‰ˆæœ¬å­—æ®µåï¼‰
    assert "WARN" in result["stderr"], "åº”è¯¥æœ‰è­¦å‘Šæ—¥å¿—æç¤ºä½¿ç”¨æ—§ç‰ˆæœ¬å­—æ®µå"
    assert "camelCase" in result["stderr"], f"è­¦å‘Šæ¶ˆæ¯åº”è¯¥æåˆ°camelCase: {result['stderr']}"
    print(f"âœ… è­¦å‘Šæ—¥å¿—æ­£ç¡®: {result['stderr'].strip()}")

    # éªŒè¯ metrics.docs_read ä»ç„¶è¢«æ›´æ–°
    task_meta = load_task_meta()
    docs_read = task_meta["metrics"]["docs_read"]

    assert len(docs_read) == 1, f"docs_read åº”è¯¥åŒ…å«1æ¡è®°å½•ï¼ˆå³ä½¿ä½¿ç”¨æ—§å­—æ®µåï¼‰"
    assert docs_read[0]["file"] == "markdown/é—®é¢˜æ’æŸ¥.md"
    print(f"âœ… æ—§å­—æ®µåä»èƒ½å·¥ä½œ: {docs_read[0]['file']}")

    print("\nã€æµ‹è¯•2é€šè¿‡ã€‘âœ… å‘åå…¼å®¹æ€§æ­£å¸¸\n")
    return True


def test_multiple_reads():
    """æµ‹è¯•3: éªŒè¯å¤šæ¬¡ Read è°ƒç”¨ç´¯ç§¯è®°å½•"""
    print("\nã€æµ‹è¯•3ã€‘éªŒè¯å¤šæ¬¡ Read è°ƒç”¨èƒ½ç´¯ç§¯è®°å½•åˆ° docs_read")
    print("=" * 60)

    # é‡ç½®æµ‹è¯•ç¯å¢ƒ
    cleanup_test_environment()
    setup_test_environment()

    markdown_files = [
        "markdown/å¼€å‘è§„èŒƒ.md",
        "markdown/é—®é¢˜æ’æŸ¥.md",
        "markdown/å¿«é€Ÿå¼€å§‹.md"
    ]

    for i, file_path in enumerate(markdown_files, 1):
        hook_input = {
            "session_id": "test-session",
            "cwd": str(TEST_TASK_DIR),
            "hook_event_name": "PostToolUse",
            "tool_name": "Read",
            "tool_input": {"file_path": file_path},
            "tool_response": {"success": True},
            "is_error": False
        }

        result = run_hook(hook_input)
        assert result["returncode"] == 0

        task_meta = load_task_meta()
        docs_read = task_meta["metrics"]["docs_read"]

        assert len(docs_read) == i, f"ç¬¬{i}æ¬¡Readååº”è¯¥æœ‰{i}æ¡è®°å½•ï¼Œå®é™…: {len(docs_read)}"
        print(f"  âœ… ç¬¬{i}æ¬¡ Read: {file_path} â†’ docs_read.length = {len(docs_read)}")

    # æœ€ç»ˆéªŒè¯
    task_meta = load_task_meta()
    docs_read = task_meta["metrics"]["docs_read"]

    assert len(docs_read) == 3, "æœ€ç»ˆåº”è¯¥æœ‰3æ¡è®°å½•"
    assert all(doc["file"] in markdown_files for doc in docs_read)

    print(f"\nâœ… ç´¯ç§¯è®°å½•æ­£ç¡®: {[doc['file'] for doc in docs_read]}")
    print("\nã€æµ‹è¯•3é€šè¿‡ã€‘âœ… å¤šæ¬¡Readç´¯ç§¯è®°å½•æ­£å¸¸\n")
    return True


def test_non_markdown_ignored():
    """æµ‹è¯•4: éªŒè¯émarkdownæ–‡ä»¶ä¸ä¼šè¢«è®°å½•åˆ° docs_read"""
    print("\nã€æµ‹è¯•4ã€‘éªŒè¯émarkdownæ–‡ä»¶ä¸è®°å½•åˆ° docs_read")
    print("=" * 60)

    # é‡ç½®æµ‹è¯•ç¯å¢ƒ
    cleanup_test_environment()
    setup_test_environment()

    # è¯»å–Pythonæ–‡ä»¶ï¼ˆä¸åº”è¯¥è®°å½•ï¼‰
    hook_input = {
        "session_id": "test-session",
        "cwd": str(TEST_TASK_DIR),
        "hook_event_name": "PostToolUse",
        "tool_name": "Read",
        "tool_input": {"file_path": "behavior_packs/test.py"},
        "tool_response": {"success": True},
        "is_error": False
    }

    result = run_hook(hook_input)
    assert result["returncode"] == 0

    task_meta = load_task_meta()
    docs_read = task_meta["metrics"]["docs_read"]

    assert len(docs_read) == 0, f"Pythonæ–‡ä»¶ä¸åº”è¯¥è®°å½•åˆ°docs_readï¼Œå®é™…: {len(docs_read)}"
    print("âœ… Pythonæ–‡ä»¶æœªè®°å½•åˆ° docs_read")

    # tools_used åº”è¯¥ä»ç„¶è®°å½•
    tools_used = task_meta["metrics"]["tools_used"]
    assert len(tools_used) == 1
    assert tools_used[0]["tool"] == "Read"
    print("âœ… tools_used ä»ç„¶è®°å½•äº†Readå·¥å…·")

    print("\nã€æµ‹è¯•4é€šè¿‡ã€‘âœ… émarkdownæ–‡ä»¶è¿‡æ»¤æ­£å¸¸\n")
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("PostToolUse Hook å­—æ®µåä¿®å¤éªŒè¯æµ‹è¯•å¥—ä»¶")
    print("=" * 60)

    try:
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        setup_test_environment()

        # è¿è¡Œæµ‹è¯•
        tests = [
            ("snake_case å­—æ®µå", test_snake_case_fields),
            ("camelCase å‘åå…¼å®¹", test_camelcase_backward_compatibility),
            ("å¤šæ¬¡Readç´¯ç§¯", test_multiple_reads),
            ("émarkdownè¿‡æ»¤", test_non_markdown_ignored)
        ]

        results = []
        for name, test_func in tests:
            try:
                test_func()
                results.append((name, True, None))
            except Exception as e:
                results.append((name, False, str(e)))
                print(f"âŒ æµ‹è¯•å¤±è´¥: {name}")
                print(f"   é”™è¯¯: {e}\n")

        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 60)
        print("æµ‹è¯•æ€»ç»“")
        print("=" * 60)

        passed = sum(1 for _, success, _ in results if success)
        total = len(results)

        for name, success, error in results:
            status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
            print(f"{status}: {name}")
            if error:
                print(f"       é”™è¯¯: {error}")

        print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")

        if passed == total:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼BUGä¿®å¤éªŒè¯æˆåŠŸï¼")
            return 0
        else:
            print(f"\nâš ï¸ æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
            return 1

    finally:
        # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        cleanup_test_environment()


if __name__ == "__main__":
    sys.exit(run_all_tests())
